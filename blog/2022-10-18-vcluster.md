## 1 vcluster 介绍
虚拟集群（virtual cluster, 简称 vcluster）是在常规的 Kubernetes 集群之上运行的一个功能齐全，轻量级，隔离性良好的 Kubernetes 集群。与完全独立的“真实“集群相比，虚拟集群没有自己的节点池，但是可以在底层宿主集群上调度工作负载，同时具有自己单独的控制平面。

![](https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20220922222305.png)

vcluster 有以下特点：
- **可使用集群层面的资源**：在虚拟集群允许租户使用 CRD、Namespaces、ClusterRole 等资源，这比通过命名空间隔离的方式功能更加强大。
-   **轻量级**：vcluster 默认使用 k3s 构建虚拟集群，k3s 是一个经过认证的轻量级 Kubernetes 发行版，100% 兼容 Kubernetes API，它将 Kubernetes 组件编译为小于 100 MB 的单个二进制文件中，默认禁用所有不需要的 Kubernetes 功能，例如 pod 调度程序或某些控制器，这使得 k3s 的内存使用仅仅为常规 k8s 的一半。另外 vcluster 还支持其他发行版，例如 k0s 或常规 k8s。
- **经济高效**：创建虚拟集群比“真正的“集群更加便宜和高效，最少只需要创建单个 vcluster pod（包含 API server, syncer, 后端存储）。
- **良好的隔离性**：每个虚拟集群有独立的控制面，只是共享了底层宿主集群的计算资源等服务。
-   **没有性能下降**：Pod 实际上被部署在底层主机集群中，因此它们在运行时根本不会受到性能影响。
-   **减少主机集群的开销**：将大型多租户集群拆分为更小的 vcluster，以降低复杂性并提高可扩展性。由于大多数 vcluster api 请求和对象根本不会到达宿主集群，因此 vcluster 可以大大降低底层 Kubernetes 集群的压力
-   **易于部署** ：vcluster 可以通过 vcluster CLI、helm、kubectl、[cluster api](https://github.com/loft-sh/cluster-api-provider-vcluster)、Argo CD 等多种工具进行部署（它基本上只是一个 StatefulSet 资源）。
-   **单一命名空间封装**：每个 vcluster 及其所有的工作负载都位于底层宿主集群的单一命名空间内。
-   **灵活和多功能**：vcluster 支持不同的后端存储（例如 sqlite、mysql、postgresql 和 etcd）、插件、自定义资源同步策略，你甚至可以在 vcluster 中部署 vcluster。

使用虚拟集群相比创建单独的 Kubernetes 集群更经济高效，相较于常规的命名空间则能够提供更好的多租户和隔离特性。下表对命名空间、vcluster 和 Kubernetes 集群 3 种方式在隔离性、多租户访问、成本等方面进行了对比。

![](https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20221015164752.png)
## 2 vcluster 快速上手
### 2.1 准备持久化存储
创建 vcluster 默认需要使用持久化存储，如果集群上已经配置好默认的持久化存储，可以跳过此步骤。

执行以下命令，安装 OpenEBS。
```bash
kubectl apply -f https://openebs.github.io/charts/openebs-operator.yaml
```

设置 StorageClass openebs-hostpath 作为默认的 StorageClass。
```bash
kubectl patch storageclass openebs-hostpath -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
```

![](https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20221015173730.png)

确认 OpenEBS 各组件正常运行。

![](https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20220922214231.png)

### 2.2 安装 vcluster CLI
参照 [Install vcluster CLI[1]](https://www.vcluster.com/docs/getting-started/setup) 根据对应的操作系统版本进行安装。

### 2.3 创建虚拟集群
执行以下命令创建一个名为 my-vcluster 的虚拟集群，默认会在 `vcluster-<vcluster-name>` （本例中是 vcluster-my-vcluster）Namespace 中部署虚拟集群，也可以使用 `-n` 参数指定部署虚拟集群的 Namespace。
```bash
vcluster create my-vcluster
```

虚拟集群创建成功后，vcluster 会自动帮我们通过端口转发连接到虚拟集群。如果使用 kubectl 或者 helm 的方式安装虚拟集群，则可以使用 `vcluster connect <cluster-name>` 命令来手动连接到虚拟集群。

![](https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20220922215010.png)

打开另一个窗口执行 kubectl 命令查看 Pod 和 Namespace，可以看到这是一个全新的集群，并不能看到虚拟集群所属的 vcluster-my-vcluster Namespace，因为该 Namespace 存在于宿主集群中。 

![](https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20220922215322.png)

### 2.4 在虚拟集群中创建资源
在虚拟集群中创建一个 Namespace，并在里面部署一个 nginx Deployment。
```bash
kubectl create namespace demo-nginx
kubectl create deployment nginx-deployment -n demo-nginx --image=nginx
```

查看创建的 Pod。
```bash
> kubectl get pod -n demo-nginx
NAME                                READY   STATUS    RESTARTS   AGE
nginx-deployment-5fbdf85c67-42rmp   1/1     Running   0          13s
```

键盘按 ctrl + c 断开和虚拟集群的连接，kubectl 的上下文会自动切换回宿主集群。

![](https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20220922220731.png)


在宿主集群查看 Namespace，并没有看到在虚拟集群中创建的 demo-nginx Namespace，因为该 Namespace 只存在于虚拟集群中。

![](https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20220922220954.png)

同样也看不到 nginx-deployment。

![](https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20220922221516.png)


Pod 在虚拟集群所属的 Namespace 中是存在的，vcluster 中有一个 **syncer** 控制器，主要负责将虚拟集群中的资源同步到底层宿主集群的命名空间中，Pod 的实际调度还是依靠宿主集群上的调度器完成的。

![](https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20220922221337.png)

### 2.5 清理虚拟集群
使用 `vcluster delete` 命令可以删除虚拟集群。
```bash
vcluster delete my-vcluster
```
## 3 暴露 vcluster
默认情况下，vcluster 只能通过远程集群中的端口转发进行访问。要想直接访问虚拟集群可以选择通过使用 LoadBalancer 或者 NodePort 类型的 Service 将虚拟集群暴露到集群外。

最简单的方式就是在创建虚拟集群的时候指定 `--expose` 参数，vcluster 会创建 LoadBalancer 类型的 Service 暴露虚拟集群（前提要有公有云托管的 Kubernetes 集群支持 LoadBalancer）。等待虚拟集群创建完成后，vcluster 会自动帮我们将虚拟集群的 kubeconfig 配置添加到 ~/.kube/config 文件中，此时可以直接通过 kubectl 命令行访问虚拟集群。
```bash
vcluster create my-vcluster --expose
```

你也可以手动创建 Service 来暴露 vcluster，更多方式参见 [Exposing vcluster (ingress etc.)](https://www.vcluster.com/docs/operator/external-access)。

## 4 网络 & DNS 解析
默认情况下，Pod 和 Service 资源都会从虚拟集群同步到主机集群，以便为 vcluster 启用正确的网络功能。

![](https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20221015190051.png)

创建一个虚拟集群用于测试。
```bash
vcluster create net-vcluster
```
### 4.1 通过 IP 地址通信
在虚拟集群中创建的 Pod 会被 vcluster 的 syncer 同步到宿主集群中，因此 Pod 实际上运行在底层宿主集群中。这意味着这些 Pod 具有常规的集群内部 IP 地址，并且可以通过 IP 地址相互通信。

在虚拟集群中创建一个 Pod。
```bash
kubectl create deployment nettool-virtual --image=cr7258/nettool:v1
```
在宿主集群中创建一个 Pod。（在宿主集群 context 中执行）
```bash
# 打开另一个窗口，先切换到宿主集群的上下文中，可以使用 kubectl config get-context 命令列出上下文
kubectl config use-context <host-cluster-context>
kubectl create deployment nettool-host --image=cr7258/nettool:v1
```

查看在虚拟集群和宿主集群中创建的 Pod 的 IP 地址。（在宿主集群 context 中执行）
```bash
kubectl get pod -o wide
kubectl get pod -n vcluster-net-vcluster -o wide
```

![](https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20220923110715.png)

两个 Pod 之间互相 Ping 测试，网络之间可以互通。

![](https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20220923114034.png)

虚拟集群和宿主集群之间的 Pod 以及 Service 资源默认情况下都可以直接通过 IP 地址互相访问。

### 4.2 通过域名通信
每个虚拟集群都有自己独立的 DNS 服务（CoreDNS），为虚拟集群中的 Service 提供 DNS 解析。

将虚拟集群和宿主集群的 Deployment 分别通过 Service 进行暴露。
```bash
kubectl config use-context <host-cluster-context>
kubectl expose deployment nettool-host --port=80 --target-port=80

kubectl config use-context <virtual-cluster-context>
kubectl expose deployment nettool-virtual --port=80 --target-port=80
```
### 4.3 将宿主集群 Service 映射到虚拟集群中
将宿主集群 default Namespace 中的 nettool-host Service 映射到虚拟集群的 default Namespace 中的 nettool-host Service。
```yaml
mapServices:
  fromHost:
  - from: default/nettool-host 
    to: default/nettool-host
```
```bash
vcluster create net-vcluster --upgrade -f host-to-vcluster.yaml
```

![](https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20220923143358.png)

![](https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20220925165954.png)

使用虚拟集群的 Pod 访问宿主集群的 nettool-host Service。
```bash
> kubectl exec -it deployments/nettool-virtual -- curl nettool-host

# 返回结果
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
```
### 4.4 将虚拟集群 Service 映射到宿主集群中
```bash
vcluster create net-vcluster --upgrade -f vcluster-to-host.yaml
```


![](https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20220925170048.png)

使用宿主集群的 Pod 访问虚拟集群的 nettool-virtual Service，虚拟集群中的 Service 会映射到宿主集群的 vcluster-net-vcluster Namespace 中。
```bash
> kubectl exec -it deployments/nettool-host -- curl nettool-virtual.vcluster-net-vcluster

# 返回结果
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
```

实验完毕后，执行以下命令清理虚拟集群。
```bash
vcluster delete net-vcluster
```

## 5 暂停 & 恢复虚拟集群
创建一个虚拟集群用于测试。
```bash
vcluster create recover-vcluster
```

查看当前虚拟集群运行的工作负载：
- coredns Pod 会通过 syncer 从宿主集群同步到虚拟集群中。
- recover-vcluster 以 StatefulSet 的方式部署，用于管理虚拟集群。

![](https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20221004234534.png)

### 5.1 暂停虚拟集群
执行以下命令，暂停虚拟集群。会将 vcluster 的 StatefulSet 的副本数缩减为 0，并删除 vcluster 创建的所有工作负载（本示例中是 coredns Pod）。

```bash
kubectl config use-context <host-cluster-context>
vcluster pause recover-vcluster
```

![](https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20221004234640.png)

### 5.2 恢复虚拟集群
执行以下命令，恢复虚拟集群。会将 vcluster 的 StatefulSet 的副本数恢复为原样，并且 vcluster syncer 将重新创建相应的工作负载。

```bash
vcluster resume recover-vcluster
```

![](https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20221004234739.png)

实验完毕后，执行以下命令清理虚拟集群。
```bash
vcluster delete recover-vcluster
```

## 6 存储

![](https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20221005000122.png)

```bash
# sync-storage.yaml
vcluster create storage-vcluster -f sync-storage.yaml
```

### 6.1 创建 StorageClass
```yaml
# sc.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: my-local-hostpath
  annotations:
    openebs.io/cas-type: local
    cas.openebs.io/config: |
      - name: StorageType
        value: hostpath
      - name: BasePath
        value: /var/my-local-hostpath
provisioner: openebs.io/local # 指定 OpenEBS 作为持久卷的 Provisioner
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
```

在虚拟集群中创建 StorageClass。
```bash
kubectl config use-context <virtual-cluster-context>
kubectl apply -f sc.yaml
```

vcluster 会在宿主集群中创建真正的 StorageClass，将 my-local-hostpath StorageClass 以某种格式进行重写。

```bash
kubectl config use-context <host-cluster-context>
kubectl get sc | grep my-local-hostpath
```

![](https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20221005202932.png)

### 6.2 创建 PersistentVolumeClaim
```yaml
# pvc-sc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-persistent-volume-claim
spec:
  storageClassName: my-local-hostpath # 指定 StorageClass
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
```

在虚拟集群中创建 PersistentVolumeClaim。
```bash
kubectl config use-context <virtual-cluster-context>
kubectl apply -f pvc-sc.yaml
```

由于我们创建的 StorageClass 将 `volumeBindingMode` 参数设置为 `WaitForFirstConsumer`，表示当 PVC 被 Pod 使用时，才触发 PV 和后端存储的创建，同时实现 PVC/PV 的绑定，由于当前还没有 Pod 使用该 PVC，因此 PVC 当前处于 Pending 状态。如果要让 PVC 立即和 PV 进行绑定，可以在 StorageClass 中将 `volumeBindingMode` 参数设置为 `Immediate`。

![](https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20221005203646.png)


查看宿主集群中真正创建的 PVC。
```bash
kubectl config use-context <host-cluster-context>
kubectl get pvc -n vcluster-storage-vcluster
```

![](https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20221005204240.png)


### 6.3 创建 Pod 消费 PersistentVolumeClaim
```yaml
# pod-sc.yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
    - name: my-pod
      image: nginx
      volumeMounts:
        - name: config
          mountPath: /usr/share/nginx/html
          subPath: html
  volumes:
    - name: config
      persistentVolumeClaim:
        claimName: my-persistent-volume-claim # 消费 PVC
```

在虚拟集群中创建 Pod。
```bash
kubectl config use-context <virtual-cluster-context>
kubectl apply -f pvc-sc.yaml
```

可以看到当前在虚拟集群中的 Pod 已经成功 Running，并且 PVC 也绑定了 PV。

![](https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20221005205314.png)


实验完毕后，执行以下命令清理虚拟集群。

```bash
vcluster delete storage-vcluster
```


## 7 高可用
由于国内无法直接拉去 gcr 的镜像，这里我提前将相关镜像拉取到我的 Docker Hub 上，大家可以直接使用。
```yaml
# ha.yaml

# Enable HA mode
enableHA: true

# Scale up syncer replicas
syncer:
  replicas: 3

# Scale up etcd
etcd:
  image: cr7258/k8s.gcr.io.etcd:3.5.4-0
  replicas: 3
  storage:
    # If this is disabled, vcluster will use an emptyDir instead
    # of a PersistentVolumeClaim
    persistence: false
    
# Scale up controller manager
controller:
  image: cr7258/k8s.gcr.io.kube-controller-manager:v1.25.0
  replicas: 3

# Scale up api server
api:
  image: cr7258/k8s.gcr.io.kube-apiserver:v1.25.0
  replicas: 3

# Scale up DNS server
coredns:
  replicas: 3
```

- `--connect=false` 表示不切换到虚拟集群所在的 kubeconfig 上下文。
- `--distro` 参数可以指定创建虚拟集群使用的 Kubernetes 发行版，默认使用 K3S 作为虚拟集群，这里我们指定使用 Vanilla k8s （普通的 Kubernetes 发行版）来部署虚拟集群。
```bash
vcluster create ha-vcluster --connect=false --distro k8s -f ha.yaml
```

查看创建的虚拟集群控制平面 Pod。可以看到虚拟集群控制平面的组件都有 3 个。

![](https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20221009180734.png)

实验完毕后，执行以下命令清理虚拟集群。

```bash
vcluster delete ha-vcluster
```

## 8 Pod 调度

默认情况下，vcluster 将重用主机集群的调度程序来调度工作负载。这样可以节省计算资源，但也有一些限制：
- 1.在虚拟集群内标记节点对调度没有影响
- 2.虚拟集群内的排空或污染节点对调度没有影响
- 3.您不能在 vcluster 中使用自定义调度程序

```yaml
# schedule.yaml
sync:
  nodes:
    enabled: true
    enableScheduler: true
    syncAllNodes: true
```

```bash
vcluster create schedule-vcluster -f schedule.yaml
```

查看虚拟集群的节点。vcluster 的节点有以下几种模式：
- **Fake Nodes**
- **Real Nodes**
- **Real Nodes All**
- **Real Nodes Label Selector**
```bash
> kubectl get node
NAME                             STATUS   ROLES    AGE   VERSION
ip-192-168-29-123.ec2.internal   Ready    <none>   20m   v1.23.9-eks-ba74326
ip-192-168-44-166.ec2.internal   Ready    <none>   20m   v1.23.9-eks-ba74326
```

给节点 ip-192-168-44-166.ec2.internal 打上标签 `disktype=ssd`。
```bash
kubectl label nodes ip-192-168-44-166.ec2.internal disktype=ssd
```


创建 Deployment，通过 nodeSelector 参数根据标签选择节点，将 6 个 Pod 都分配到节点 ip-192-168-44-166.ec2.internal 上。
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 6
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
      nodeSelector:
        disktype: ssd
```

查看 Pod 的分布情况，可以看到所有的 Pod 都调度到了节点 ip-192-168-44-166.ec2.internal 上了。

![](https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20221009183015.png)

实验完毕后，执行以下命令清理虚拟集群。

```bash
vcluster delete schedule-vcluster
```

## 9 隔离模式
```bash
vcluster create isolate-vcluster --isolate
```
### 9.1 网络隔离
在虚拟集群中创建一个 Pod。
```bash
kubectl create deployment nettool-virtual --image=cr7258/nettool:v1
```
在宿主集群中创建一个 Pod。（在宿主集群 context 中执行）
```bash
kubectl config use-context <host-cluster-context>
kubectl create deployment nettool-host --image=cr7258/nettool:v1
```

查看在虚拟集群和宿主集群中创建的 Pod 的 IP 地址。（在宿主集群 context 中执行）
```bash
kubectl get pod -o wide
kubectl get pod -n vcluster-isolate-vcluster -o wide
```

![](https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20221014191123.png)

两个 Pod 之间互相 Ping 测试，可以看到虚拟集群无法通过 IP 地址访问宿主集群，但是宿主集群可以访问虚拟集群。

![](https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20221014191533.png)

让我们看看在宿主集群中创建的 NetworkPolicy（在虚拟集群中是没有 NetworkPolicy 的）。
```bash
kubectl get networkpolicies -n vcluster-isolate-vcluster
```

![](https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20221014193641.png)

这两条 NetworkPolicy 的 YAML 文件如下所示，可以看到 NetworkPolicy 对虚拟集群的 Egress 方向的流量进行了限制，确保虚拟集群中的工作负载无法主动访问宿主集群或者其他虚拟集群。
```yaml
# 允许虚拟集群的控制平面访问宿主集群中的 CoreDNS 以及 API Server
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  annotations:
    meta.helm.sh/release-name: isolate-vcluster
    meta.helm.sh/release-namespace: vcluster-isolate-vcluster
  labels:
    app.kubernetes.io/managed-by: Helm
  name: isolate-vcluster-control-plane
  namespace: vcluster-isolate-vcluster
spec:
  egress:
  - ports:
    - port: 443
      protocol: TCP
    - port: 8443
      protocol: TCP
    - port: 6443
      protocol: TCP
  - to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: kube-system
      podSelector:
        matchLabels:
          k8s-app: kube-dns
  podSelector:
    matchLabels:
      release: isolate-vcluster
  policyTypes:
  - Egress

# 允许虚拟集群中的工作负载访问虚拟集群的控制平面，以及公网 IP（ipBlock 排除了内网 IP）
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  annotations:
    meta.helm.sh/release-name: isolate-vcluster
    meta.helm.sh/release-namespace: vcluster-isolate-vcluster
  labels:
    app.kubernetes.io/managed-by: Helm
  name: isolate-vcluster-workloads
  namespace: vcluster-isolate-vcluster
spec:
  egress:
  - ports:
    - port: 443
      protocol: TCP
    - port: 8443
      protocol: TCP
    to:
    - podSelector:
        matchLabels:
          release: isolate-vcluster
  - ports:
    - port: 53
      protocol: UDP
    - port: 53
      protocol: TCP
  - to:
    - podSelector:
        matchLabels:
          vcluster.loft.sh/managed-by: isolate-vcluster
    - ipBlock:
        cidr: 0.0.0.0/0
        except:
        - 100.64.0.0/10
        - 127.0.0.0/8
        - 10.0.0.0/8
        - 172.16.0.0/12
        - 192.168.0.0/16
  podSelector:
    matchLabels:
      vcluster.loft.sh/managed-by: isolate-vcluster
  policyTypes:
  - Egress
```

https://orca.tufin.io/netpol/

![](https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20221014192953.png)

![](https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20221014193259.png)

### 9.2 资源限制
同时 vcluster 也会在虚拟集群所在的 Namespace 创建 ResourceQuota 和 LimitRange 来限制资源的使用。
- 其中 ResourceQuota 用于控制整个虚拟集群消耗宿主集群的资源上限。默认创建的 ResourceQuota 如下所示，限制了虚拟集群最多创建 100 个 Configmap，40 个 Endpoints，最多使用 40 Gi 内存，最多使用 10 核 CPU 等等...
```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  annotations:
    meta.helm.sh/release-name: isolate-vcluster
    meta.helm.sh/release-namespace: vcluster-isolate-vcluster
  labels:
    app.kubernetes.io/managed-by: Helm
  name: isolate-vcluster-quota
  namespace: vcluster-isolate-vcluster
spec:
  hard:
    count/configmaps: "100"
    count/endpoints: "40"
    count/persistentvolumeclaims: "20"
    count/pods: "20"
    count/secrets: "100"
    count/services: "20"
    limits.cpu: "20"
    limits.ephemeral-storage: 160Gi
    limits.memory: 40Gi
    requests.cpu: "10"
    requests.ephemeral-storage: 60Gi
    requests.memory: 20Gi
    requests.storage: 100Gi
    services.loadbalancers: "1"
    services.nodeports: "0"
```
- LimitRange 用于控制每个 Pod 申请资源的上限（当创建的 Pod 没有指定 `resources.requests` 和 `resources.limits` 参数时会应用 LimitRange 的设置）。默认创建的 LimitRange 如下所示。
```yaml
apiVersion: v1
kind: LimitRange
metadata:
  annotations:
    meta.helm.sh/release-name: isolate-vcluster
    meta.helm.sh/release-namespace: vcluster-isolate-vcluster
  labels:
    app.kubernetes.io/managed-by: Helm
  name: isolate-vcluster-limit-range
  namespace: vcluster-isolate-vcluster
spec:
  limits:
  - default:
      cpu: "1"
      ephemeral-storage: 8Gi
      memory: 512Mi
    defaultRequest:
      cpu: 100m
      ephemeral-storage: 3Gi
      memory: 128Mi
    type: Container
```
实验完毕后，执行以下命令清理虚拟集群。

```bash
vcluster delete isolate-vcluster
```

## 10 Cluster API Provider
https://github.com/loft-sh/cluster-api-provider-vcluster

安装 vcluster provider。
```bash
clusterctl init --infrastructure vcluster
```
![](https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20221014225826.png)

将会部署以下资源。

![](https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20221014225922.png)


生成 vcluster 资源清单文件并部署。
```bash
export CLUSTER_NAME=vcluster
export CLUSTER_NAMESPACE=vcluster
export KUBERNETES_VERSION=1.23.0
export HELM_VALUES=""

kubectl create namespace ${CLUSTER_NAMESPACE}
clusterctl generate cluster ${CLUSTER_NAME} \
    --infrastructure vcluster \
    --kubernetes-version ${KUBERNETES_VERSION} \
    --target-namespace ${CLUSTER_NAMESPACE} | kubectl apply -f -
```

等待虚拟集群创建成功。
```bash
kubectl wait --for=condition=ready vcluster -n $CLUSTER_NAMESPACE $CLUSTER_NAME --timeout=300s
```

## 11 参考资料
- [1] [Install vcluster CLI: https://www.vcluster.com/docs/getting-started/setup
- [ ] https://www.vcluster.com/docs/getting-started/deployment
